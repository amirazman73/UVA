{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rHNReNh1wmi",
        "outputId": "bb9882d7-f294-4e17-ee15-1463dced5afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5bd64a1cbacc4bf43c089f98483d0d65576351dba5f9b58f8c53c1e07926cea6\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (0.9.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install duckdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ldooTqu813A4",
        "outputId": "0d134198-2e70-4e9e-a538-55cb79283473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "mDTe3LSQ13hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col, create_map, collect_list, concat_ws, countDistinct, avg, row_number, count, sum as _sum, max\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"PySpark_DuckDB_Project\").getOrCreate()\n",
        "\n",
        "import duckdb\n",
        "conn = duckdb.connect(database=':memory:', read_only=False)"
      ],
      "metadata": {
        "id": "Ffmv51wp14TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **READING DATA**"
      ],
      "metadata": {
        "id": "OVxPH77117l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Movie Classification Project\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path to the folder containing the data on Google Drive\n",
        "path_to_data_folder = \"/content/drive/My Drive/Big Data/Data/\"\n",
        "\n",
        "df_train_combined = spark.read.option(\"header\", \"true\").csv(f\"{path_to_data_folder}/train-1.csv\")\n",
        "\n",
        "# Loop through the rest of the training CSV files and union them with the initial DataFrame\n",
        "for i in range(2, 9):\n",
        "    df_temp = spark.read.option(\"header\", \"true\").csv(f\"{path_to_data_folder}/train-{i}.csv\")\n",
        "    df_train_combined = df_train_combined.union(df_temp)\n",
        "\n",
        "# Read all CSV files into a single DataFrame\n",
        "df_validation = spark.read.option(\"header\", \"true\").csv(f\"{path_to_data_folder}/validation_hidden.csv\")\n",
        "df_test = spark.read.option(\"header\", \"true\").csv(f\"{path_to_data_folder}/test_hidden.csv\")\n",
        "\n",
        "# Read JSON files\n",
        "json_files = [f\"{path_to_data_folder}/directing.json\", f\"{path_to_data_folder}/writing.json\"]\n",
        "df_json_writing = spark.read.json(json_files[1])\n",
        "df_json_directing = spark.read.option(\"multiLine\", True).json(json_files[0])"
      ],
      "metadata": {
        "id": "mo9eNb2B14ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EDA**\n"
      ],
      "metadata": {
        "id": "E3UJQKNn1_Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_json_writing.show()\n",
        "df_train_combined.show()\n",
        "df_validation.show()\n",
        "df_test.show()"
      ],
      "metadata": {
        "id": "ZYT1SDV91-GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_unique_writers = df_json_writing.select(\"writer\").distinct().count()\n",
        "number_of_unique_writers"
      ],
      "metadata": {
        "id": "MIHw-i1G2B-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **COMBINING DATASETS**"
      ],
      "metadata": {
        "id": "AYG15skc2CfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_json_writing_renamed = df_json_writing.withColumnRenamed(\"movie\", \"tconst\")\n",
        "df_writers_per_movie = df_json_writing_renamed.groupBy(\"tconst\").agg(countDistinct(\"writer\").alias(\"num_writers\"))\n",
        "df_movies_per_writer = df_json_writing_renamed.groupBy(\"writer\").agg(countDistinct(\"tconst\").alias(\"num_movies\"))\n",
        "\n",
        "df_movie_writer_info = df_json_writing_renamed.join(df_movies_per_writer, \"writer\")\n",
        "df_avg_movies_per_writer = df_movie_writer_info.groupBy(\"tconst\").agg(avg(\"num_movies\").alias(\"avg_movies_per_writer\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "1A2mAWjr2Der"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experience\n",
        "df_train_combined_no_label = df_train_combined.drop(\"label\")\n",
        "\n",
        "combined_df = df_train_combined_no_label.unionByName(df_validation).unionByName(df_test)\n",
        "df_json_writing_renamed = df_json_writing.withColumnRenamed(\"movie\", \"tconst\")\n",
        "\n",
        "expercience_df = df_json_writing_renamed.join(combined_df, \"tconst\", \"left\")\n",
        "\n",
        "movies_per_writer_per_year = expercience_df.groupBy(\"writer\", \"startYear\")\\\n",
        "                                           .agg(count(\"tconst\").alias(\"movies_this_year\"))\n",
        "\n",
        "# Step 2: Calculate the cumulative sum of movies for each writer over the years\n",
        "windowSpec = Window.partitionBy(\"writer\").orderBy(\"startYear\")\\\n",
        "                   .rangeBetween(Window.unboundedPreceding, 0)\n",
        "\n",
        "cumulative_movies = movies_per_writer_per_year.withColumn(\"experience\", _sum(\"movies_this_year\").over(windowSpec))\n",
        "\n",
        "# Step 3: Join this back to the original DataFrame to add the \"experience\" column\n",
        "result_df = expercience_df.join(cumulative_movies.select(\"writer\", \"startYear\", \"experience\"),\n",
        "                                on=[\"writer\", \"startYear\"], how=\"left\")\n",
        "\n",
        "df_avg_movies_per_writer = result_df.drop('_c0', 'primaryTitle', 'originalTitle', 'startYear', 'endYear', 'runtimeMinutes', 'numVotes')\n",
        "\n",
        "max_experience_per_movie = df_avg_movies_per_writer.groupBy(\"tconst\")\\\n",
        "    .agg(max(\"experience\").alias(\"max_experience\"))\n",
        "\n",
        "avg_experience_per_movie = df_avg_movies_per_writer.groupBy(\"tconst\")\\\n",
        "    .agg(avg(\"experience\").alias(\"avg_experience\"))\n",
        "\n",
        "# expercience_df = df_json_writing_renamed.join(df_train_combined, on='tconst', how='left')\n",
        "# expercience_df.show()"
      ],
      "metadata": {
        "id": "yPC7Mfkv2Egk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_combined = df_train_combined.join(max_experience_per_movie, \"tconst\", \"left\")\n",
        "df_train_combined = df_train_combined.join(avg_experience_per_movie, \"tconst\", \"left\")\n",
        "df_train = df_train_combined.join(df_writers_per_movie, on='tconst', how='left')\n",
        "df_validation = df_validation.join(df_writers_per_movie, 'tconst', 'left')\n",
        "df_test = df_test.join(df_writers_per_movie, 'tconst', 'left')\n",
        "\n",
        "df_train.show()"
      ],
      "metadata": {
        "id": "n6Gz6b5m2F3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MODIFYING DATA**"
      ],
      "metadata": {
        "id": "3xXy-z7g2Hun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.withColumn(\"numVotes\", col(\"numVotes\").cast(\"int\"))\n",
        "df_train = df_train.withColumn(\"runtimeMinutes\", col(\"runtimeMinutes\").cast(\"int\"))"
      ],
      "metadata": {
        "id": "inOWMF452GuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in NULLS for numVotes\n",
        "filter_numvote = df_train.filter(col(\"numVotes\").isNotNull())\n",
        "numvote_rows = filter_numvote.select(\"numVotes\").collect()\n",
        "numvote_list = [row['numVotes'] for row in numvote_rows]\n",
        "numvote_median = np.median(numvote_list)\n",
        "\n",
        "df_train = df_train.fillna({\"numVotes\": numvote_median})\n",
        "\n",
        "#df_train.filter(col(\"numVotes\").isNotNull()).count()\n",
        "# Check if equal to 7959"
      ],
      "metadata": {
        "id": "P_o_K7Rr2JbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in NULLS for runTime\n",
        "filter_runtime = df_train.filter(col(\"runtimeMinutes\").isNotNull())\n",
        "runtime_rows = filter_runtime.select(\"runtimeMinutes\").collect()\n",
        "runtime_list = [row['runtimeMinutes'] for row in runtime_rows]\n",
        "runtime_median = np.median(runtime_list)\n",
        "\n",
        "df_train = df_train.fillna({\"runtimeMinutes\": runtime_median})\n",
        "\n",
        "#df_train.filter(col(\"runtimeMinutes\").isNotNull()).count()\n",
        "# Check if equal to 7959"
      ],
      "metadata": {
        "id": "_nbGpxlB2KVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ACME Format and Combining Titles to Find Jaccard Similarity\n"
      ],
      "metadata": {
        "id": "N1i-CADR2NPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformation of original title by replacing all nulls in original title\n",
        "\n",
        "from pyspark.sql.functions import trim, lower, regexp_replace, udf, coalesce\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import re\n",
        "\n",
        "df_train = df_train \\\n",
        "    .withColumn(\"primaryTitle\", trim(col(\"primaryTitle\"))) \\\n",
        "    .withColumn(\"primaryTitle\", lower(col(\"primaryTitle\"))) \\\n",
        "    .withColumn(\"primaryTitle\", regexp_replace(col(\"primaryTitle\"), \"[^\\w\\s]\", \"\")) \\\n",
        "    .withColumn(\"primaryTitle\", regexp_replace(col(\"primaryTitle\"), \"[\\x00-\\x1F]+\", \"\"))\n",
        "\n",
        "df_train = df_train \\\n",
        "    .withColumn(\"originalTitle\", trim(col(\"originalTitle\"))) \\\n",
        "    .withColumn(\"originalTitle\", lower(col(\"originalTitle\"))) \\\n",
        "    .withColumn(\"originalTitle\", regexp_replace(col(\"originalTitle\"), \"[^\\w\\s]\", \"\")) \\\n",
        "    .withColumn(\"originalTitle\", regexp_replace(col(\"originalTitle\"), \"[\\x00-\\x1F]+\", \"\"))\n",
        "\n",
        "# Transform Original title to remove nulls and replace with Primary title\n",
        "df_train = df_train \\\n",
        "    .withColumn(\"originalTitle\", coalesce(\"originalTitle\", \"primaryTitle\"))\n",
        "\n",
        "df_train.show()"
      ],
      "metadata": {
        "id": "6K1Q8RVK2Nso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "import re\n",
        "\n",
        "# Define a UDF to calculate the Jaccard similarity\n",
        "def jaccard_similarity(str1, str2):\n",
        "    # Tokenize and create sets of words for each title, handling None values\n",
        "    words_str1 = set(re.sub(r\"[^\\w\\s]\", '', (str1 or \"\").lower()).split())\n",
        "    words_str2 = set(re.sub(r\"[^\\w\\s]\", '', (str2 or \"\").lower()).split())\n",
        "    # Calculate intersection and union\n",
        "    intersection = words_str1.intersection(words_str2)\n",
        "    union = words_str1.union(words_str2)\n",
        "\n",
        "    jaccard_index = float(len(intersection)) / len(union)\n",
        "    return jaccard_index\n",
        "\n",
        "# Register UDF\n",
        "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
        "\n",
        "df_train = df_train.withColumn(\"titleJaccardSimilarity\", jaccard_similarity_udf(df_train[\"primaryTitle\"], df_train[\"originalTitle\"]))'''"
      ],
      "metadata": {
        "id": "5Vuv5IGT2P49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#More Feature Engineering"
      ],
      "metadata": {
        "id": "aj3YS3hz2Rdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, log, length, lit, trim, lower, coalesce, when\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Assuming the Spark session is already initialized as 'spark'\n",
        "current_year = 2024\n",
        "\n",
        "# Transform movie age\n",
        "df_train = df_train \\\n",
        "    .withColumn(\"startYear\", when(col(\"startYear\") == \"\\\\N\", None).otherwise(col(\"startYear\")).cast(IntegerType())) \\\n",
        "    .withColumn(\"endYear\", when(col(\"endYear\") == \"\\\\N\", None).otherwise(col(\"endYear\")).cast(IntegerType())) \\\n",
        "    .withColumn(\"movieAge\", lit(current_year) - coalesce(col(\"startYear\"), col(\"endYear\"))) \\\n",
        "\n",
        "# Transform numvotes\n",
        "df_train = df_train \\\n",
        "    .withColumn(\"logNumVotes\", log(col(\"numVotes\") + 1)) \\\n",
        "\n",
        "# Transform title length\n",
        "df_train = df_train \\\n",
        "    .withColumn(\"titleLength\", length(col(\"primaryTitle\"))) \\\n",
        "\n",
        "# Transform run time votes interaction\n",
        "df_train = df_train \\\n",
        "    .withColumn(\"runtimeVotesInteraction\", col(\"runtimeMinutes\") * col(\"logNumVotes\")) \\\n",
        "\n",
        "# Transform Age and Runtime Interaction\n",
        "df_train = df_train \\\n",
        "    .withColumn(\"ageRuntimeInteraction\", col(\"movieAge\") * col(\"runtimeMinutes\"))\n"
      ],
      "metadata": {
        "id": "3yaRiEXW2Scr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.select(\"tconst\", \"primaryTitle\", \"originalTitle\", \"runtimeMinutes\", \"numVotes\", \"num_writers\", \"movieAge\", \"logNumVotes\", \"titleLength\", \"runtimeVotesInteraction\", \"avg_experience\", \"max_experience\").show(truncate=False)"
      ],
      "metadata": {
        "id": "y_ClvvAg2TPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, count, avg, expr, stddev\n",
        "\n",
        "# Check for missing or zero values\n",
        "missing_or_zero = df_train.select([count(when(col(c).isNull() | (col(c) == 0), c)).alias(c)\n",
        "                                   for c in [\"runtimeMinutes\", \"num_writers\", \"numVotes\", \"movieAge\", \"avg_experience\", \"max_experience\"]])\n",
        "\n",
        "missing_or_zero.show()"
      ],
      "metadata": {
        "id": "9KLhmxdG2Top"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient boosting model"
      ],
      "metadata": {
        "id": "CAvQWMzv2XVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import when, lower, trim, col\n",
        "\n",
        "# Ensure features have correct data types and no missing values\n",
        "# Changing label to 1 and 0\n",
        "df_train = df_train.withColumn(\"label\", when(lower(trim(col(\"label\"))) == \"true\", 1).otherwise(0))\n",
        "\n",
        "# Adjust input features\n",
        "inputs = [\"runtimeMinutes\", \"num_writers\", \"numVotes\", \"movieAge\"]\n",
        "\n",
        "# Convert inputs into a feature vector\n",
        "assembler = VectorAssembler(inputCols=inputs, outputCol=\"features\")\n",
        "\n",
        "# Train/Test Split\n",
        "(trainingData, testData) = df_train.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Initialize Gradient-Boosted Trees classifier\n",
        "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, gbt])\n",
        "\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "predictions = model.transform(testData)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Evaluate on test split\n",
        "test_auc = evaluator.evaluate(predictions)\n",
        "print(f\"Test Area Under ROC: {test_auc}\")\n",
        "\n",
        "# Optionally, evaluate on training data as well to check for overfitting\n",
        "training_predictions = model.transform(trainingData)\n",
        "train_auc = evaluator.evaluate(training_predictions)\n",
        "print(f\"Training Area Under ROC: {train_auc}\")\n"
      ],
      "metadata": {
        "id": "VZhds1f62YBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "# Precision\n",
        "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = precision_evaluator.evaluate(predictions)\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "# Recall\n",
        "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = recall_evaluator.evaluate(predictions)\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# F1 Score\n",
        "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1 = f1_evaluator.evaluate(predictions)\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "id": "9ckVweUN2dM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test and Validation\n",
        "\n"
      ],
      "metadata": {
        "id": "W0uHwq952fNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_test.withColumn(\"numVotes\", col(\"numVotes\").cast(\"int\"))\n",
        "df_test = df_test.withColumn(\"runtimeMinutes\", col(\"runtimeMinutes\").cast(\"int\"))\n",
        "\n",
        "# Fill in NULLS for numVotes\n",
        "filter_numvote = df_test.filter(col(\"numVotes\").isNotNull())\n",
        "numvote_rows = filter_numvote.select(\"numVotes\").collect()\n",
        "numvote_list = [row['numVotes'] for row in numvote_rows]\n",
        "numvote_median = np.median(numvote_list)\n",
        "\n",
        "df_test = df_test.fillna({\"numVotes\": numvote_median})\n",
        "\n",
        "\n",
        "# Fill in NULLS for runTime\n",
        "filter_runtime = df_test.filter(col(\"runtimeMinutes\").isNotNull())\n",
        "runtime_rows = filter_runtime.select(\"runtimeMinutes\").collect()\n",
        "runtime_list = [row['runtimeMinutes'] for row in runtime_rows]\n",
        "runtime_median = np.median(runtime_list)\n",
        "\n",
        "df_test = df_test.fillna({\"runtimeMinutes\": runtime_median})\n",
        "\n",
        "df_test = df_test \\\n",
        "    .withColumn(\"primaryTitle\", trim(col(\"primaryTitle\"))) \\\n",
        "    .withColumn(\"primaryTitle\", lower(col(\"primaryTitle\"))) \\\n",
        "    .withColumn(\"primaryTitle\", regexp_replace(col(\"primaryTitle\"), \"[^\\w\\s]\", \"\")) \\\n",
        "    .withColumn(\"primaryTitle\", regexp_replace(col(\"primaryTitle\"), \"[\\x00-\\x1F]+\", \"\"))\n",
        "\n",
        "df_test = df_test \\\n",
        "    .withColumn(\"originalTitle\", trim(col(\"originalTitle\"))) \\\n",
        "    .withColumn(\"originalTitle\", lower(col(\"originalTitle\"))) \\\n",
        "    .withColumn(\"originalTitle\", regexp_replace(col(\"originalTitle\"), \"[^\\w\\s]\", \"\")) \\\n",
        "    .withColumn(\"originalTitle\", regexp_replace(col(\"originalTitle\"), \"[\\x00-\\x1F]+\", \"\"))\n",
        "\n",
        "# Transform Original title to remove nulls and replace with Primary title\n",
        "df_test = df_test \\\n",
        "    .withColumn(\"originalTitle\", coalesce(\"originalTitle\", \"primaryTitle\"))\n",
        "\n",
        "# Transform movie age\n",
        "df_test = df_test \\\n",
        "    .withColumn(\"startYear\", when(col(\"startYear\") == \"\\\\N\", None).otherwise(col(\"startYear\")).cast(IntegerType())) \\\n",
        "    .withColumn(\"endYear\", when(col(\"endYear\") == \"\\\\N\", None).otherwise(col(\"endYear\")).cast(IntegerType())) \\\n",
        "    .withColumn(\"movieAge\", lit(current_year) - coalesce(col(\"startYear\"), col(\"endYear\"))) \\\n",
        "\n",
        "# Transform numvotes\n",
        "df_test = df_test \\\n",
        "    .withColumn(\"logNumVotes\", log(col(\"numVotes\") + 1)) \\\n",
        "\n",
        "# Transform title length\n",
        "df_test = df_test \\\n",
        "    .withColumn(\"titleLength\", length(col(\"primaryTitle\"))) \\\n",
        "\n",
        "# Transform run time votes interaction\n",
        "df_test = df_test \\\n",
        "    .withColumn(\"runtimeVotesInteraction\", col(\"runtimeMinutes\") * col(\"logNumVotes\")) \\\n",
        "\n",
        "# Transform Age and Runtime Interaction\n",
        "df_test = df_test \\\n",
        "    .withColumn(\"ageRuntimeInteraction\", col(\"movieAge\") * col(\"runtimeMinutes\"))\n",
        "\n",
        "\n",
        "# Check for missing or zero values\n",
        "missing_or_zero_test = df_test.select([count(when(col(c).isNull() | (col(c) == 0), c)).alias(c)\n",
        "                                   for c in [\"runtimeMinutes\", \"num_writers\", \"numVotes\", \"movieAge\"]])\n",
        "\n",
        "missing_or_zero_test.show()\n"
      ],
      "metadata": {
        "id": "xYAxpsVd2iuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation = df_validation.withColumn(\"numVotes\", col(\"numVotes\").cast(\"int\"))\n",
        "df_validation = df_validation.withColumn(\"runtimeMinutes\", col(\"runtimeMinutes\").cast(\"int\"))\n",
        "\n",
        "# Fill in NULLS for numVotes\n",
        "filter_numvote = df_validation.filter(col(\"numVotes\").isNotNull())\n",
        "numvote_rows = filter_numvote.select(\"numVotes\").collect()\n",
        "numvote_list = [row['numVotes'] for row in numvote_rows]\n",
        "numvote_median = np.median(numvote_list)\n",
        "\n",
        "df_validation = df_validation.fillna({\"numVotes\": numvote_median})\n",
        "\n",
        "\n",
        "# Fill in NULLS for runTime\n",
        "filter_runtime = df_validation.filter(col(\"runtimeMinutes\").isNotNull())\n",
        "runtime_rows = filter_runtime.select(\"runtimeMinutes\").collect()\n",
        "runtime_list = [row['runtimeMinutes'] for row in runtime_rows]\n",
        "runtime_median = np.median(runtime_list)\n",
        "\n",
        "df_validation = df_validation.fillna({\"runtimeMinutes\": runtime_median})\n",
        "\n",
        "df_validation = df_validation \\\n",
        "    .withColumn(\"primaryTitle\", trim(col(\"primaryTitle\"))) \\\n",
        "    .withColumn(\"primaryTitle\", lower(col(\"primaryTitle\"))) \\\n",
        "    .withColumn(\"primaryTitle\", regexp_replace(col(\"primaryTitle\"), \"[^\\w\\s]\", \"\")) \\\n",
        "    .withColumn(\"primaryTitle\", regexp_replace(col(\"primaryTitle\"), \"[\\x00-\\x1F]+\", \"\"))\n",
        "\n",
        "df_validation = df_validation \\\n",
        "    .withColumn(\"originalTitle\", trim(col(\"originalTitle\"))) \\\n",
        "    .withColumn(\"originalTitle\", lower(col(\"originalTitle\"))) \\\n",
        "    .withColumn(\"originalTitle\", regexp_replace(col(\"originalTitle\"), \"[^\\w\\s]\", \"\")) \\\n",
        "    .withColumn(\"originalTitle\", regexp_replace(col(\"originalTitle\"), \"[\\x00-\\x1F]+\", \"\"))\n",
        "\n",
        "# Transform Original title to remove nulls and replace with Primary title\n",
        "df_validation = df_validation \\\n",
        "    .withColumn(\"originalTitle\", coalesce(\"originalTitle\", \"primaryTitle\"))\n",
        "\n",
        "# Transform movie age\n",
        "df_validation = df_validation \\\n",
        "    .withColumn(\"startYear\", when(col(\"startYear\") == \"\\\\N\", None).otherwise(col(\"startYear\")).cast(IntegerType())) \\\n",
        "    .withColumn(\"endYear\", when(col(\"endYear\") == \"\\\\N\", None).otherwise(col(\"endYear\")).cast(IntegerType())) \\\n",
        "    .withColumn(\"movieAge\", lit(current_year) - coalesce(col(\"startYear\"), col(\"endYear\"))) \\\n",
        "\n",
        "# Transform numvotes\n",
        "df_validation = df_validation \\\n",
        "    .withColumn(\"logNumVotes\", log(col(\"numVotes\") + 1)) \\\n",
        "\n",
        "# Transform title length\n",
        "df_validation = df_validation \\\n",
        "    .withColumn(\"titleLength\", length(col(\"primaryTitle\"))) \\\n",
        "\n",
        "# Transform run time votes interaction\n",
        "df_validation = df_validation \\\n",
        "    .withColumn(\"runtimeVotesInteraction\", col(\"runtimeMinutes\") * col(\"logNumVotes\")) \\\n",
        "\n",
        "# Transform Age and Runtime Interaction\n",
        "df_validation = df_validation \\\n",
        "    .withColumn(\"ageRuntimeInteraction\", col(\"movieAge\") * col(\"runtimeMinutes\"))\n",
        "\n",
        "\n",
        "# Check for missing or zero values\n",
        "missing_or_zero_test = df_validation.select([count(when(col(c).isNull() | (col(c) == 0), c)).alias(c)\n",
        "                                   for c in [\"runtimeMinutes\", \"num_writers\", \"numVotes\", \"movieAge\"]])\n",
        "\n",
        "missing_or_zero_test.show()\n"
      ],
      "metadata": {
        "id": "59SJvsZC4t3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform df_test to get predictions\n",
        "test_predictions = model.transform(df_test)\n",
        "# Show some of the predictions\n",
        "test_predictions = test_predictions.withColumn(\"label\", expr(\"CASE WHEN prediction = 1 THEN 'True' ELSE 'False' END\"))\n"
      ],
      "metadata": {
        "id": "nhxFvRC62jp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform df_validation to get predictions\n",
        "val_predictions = model.transform(df_validation)\n",
        "# Show some of the predictions\n",
        "val_predictions = val_predictions.withColumn(\"label\", expr(\"CASE WHEN prediction = 1 THEN 'True' ELSE 'False' END\"))\n"
      ],
      "metadata": {
        "id": "xWLebuN-2lC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path_test = \"test_predictions.csv\"\n",
        "output_path_val = \"val_predictions.csv\"\n",
        "\n",
        "selected_columns = test_predictions.select(\n",
        "    \"label\"\n",
        ")\n",
        "\n",
        "selected_columns_val = val_predictions.select(\n",
        "    \"label\"\n",
        ")\n",
        "\n",
        "# Save the DataFrame to a CSV file without headers\n",
        "selected_columns.coalesce(1).write.mode(\"overwrite\").csv(output_path_test, header=False)\n",
        "selected_columns_val.coalesce(1).write.mode(\"overwrite\").csv(output_path_val, header=False)"
      ],
      "metadata": {
        "id": "fpcuUgw82mnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}